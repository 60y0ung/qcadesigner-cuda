\chapter{Implementation}\label{sec:implementation}
\section{First approach}
The original source code of QCADesigner was downloaded from Mina website (ref). We attempted to make it compile as it was but we did not
manage to solve several compilation errors. So we started to focus on the identification of the core algorithm supporting the tool in
order to obtain a working batch simulator executable on CPU. This operation took us some weeks of work. Meanwhile we were able to deeply
analyze the code. We made some hypothesis on the location of possible bottlenecks, we identified the data structures used to represent 
circuits and started to consider possible transformations that had to be done in order to obtain fast accessible and light weight data 
structures allocable on the GPU global memory.

\section{The CPU algorithm and code analysis}\label{sec:cpu_algorithm}
Bistable engine is thought as a fast and approximated simulation, sufficient to verify the logic functionality of a design. 
Every cell is represented as a simple two-state system. The entire simulation is divided into samples, that are units of time 
(not yet experimentally defined), and for each sample the state of each cell is calculated with respect to the other cells within 
a preset effective radius. This operation is iterated until all cells have converged within a predetermined tolerance. 
Once the entire system converges the output is recorded and the computation goes on with the next sample, after having updated
 the input cells with new input values.
The number of samples required to have a good approximation is known (ref) to be $2000*2^N$, where $N$ is the number of inputs.
 The maximum number of iterations allowed for the convergence within a sample is set to $100$. Thus, during a simulation each cell's value 
is computed sequentially $It*2000*2^N$ times, where $It$ is the mean number of iterations needed to reach convergence. 
The more are the cells, the longer will take each sample to reach convergence. Each cell's new state calculation implies several accesses to the memory, since data structures are heavily dereferenced, and each neighbor's state as well as their reciprocal kink energy has to be read, and a series of floating point operations on values with double precision. Furthermore, at the beginning of each sample, new inputs values have to be set. As far as exhaustive simulation is concerned, every combination of input values has to be processed. Therefore, their values are calculated through a periodic function, implying several transcendental functional unit usages.\newline
The analysis of the code makes it quite clear that the core of the simulation is the main bottleneck of this application. Once a circuit of millions of cells have to be simulated, the number of FP operations and memory accesses reach huge orders of magnitude. This first hypothesis was subsequently proved by the profiling of execution times, dealt with in section \ref{sec:cpu_profiling}.

\section{Profiling of CPU simulation}\label{sec:cpu_profiling}
Once the batch application was finished we started to profile the execution times simulating some circuits of different sizes. The table \ref{tab:cpu_profiling} shows ...\newline
\begin{table}[h!tb]
   \centering \caption{Table caption}
   \label{tab:cpu_profiling}
   \vskip 0.2cm
   %%
   \scalebox{0.90}{
	    %% The {|c|c|c|c|c|} define the number of columns.
	    %% c means centered
	    %% | defines a vertical line between two columns 
	    \begin{tabular}{|c|c|c|}
	      \hline
	      Col 1 & Col 2 & Col2  \\
	      %% \\ force a newline without creating an horizontal line
	      Dim C.1 & Dim C.3 & Dim C.3 \\
	      %% \hline create an horizontal line between two rows
	      \hline
	      Data 1.1 & Data 1.2 & Data 1.3 \\
	      \hline
	      Data 2.1 & Data 2.2 & Data 2.3 \\
	      \hline
	      Data 3.1 & Data 3.2 & Data 3.3 \\  
	      \hline
	    \end{tabular}
	 }
 \end{table}
-> CALCOLARE LA PERCENTUALE DEL TEMPO IN FUNZIONE DELLE CELLE E DEGLI INPUT (alla buona) + un po' di analisi a confermare le cose speculate a caso nella sezione precedente + BLABLA


\section{Parallelization Strategy}
\subsection{The new algorithm and achievable speedup}\label{sec:new_algorithm}
As we have seen in section \ref{sec:cpu_algorithm}, the calculation of the each cell's new state is immediately stored before proceeding with the next cell. Therefore, not all of the new polarizations computation is based on the old value of neighbors. This dependence challenges our seek of the maximum speedup, since our initial proposal was to compute every cell's new state simultaneously. The figure \ref{fig:cpu_alg} shows how this dependency affects a simulation step varying the order in which the cells are considered.
\begin{figure}[h!tb]
	\centering
	\subfigure[First order]{\includegraphics[scale=0.4]{img/CPUalg1.png}}
	\subfigure[Second order]{\includegraphics[scale=0.4]{img/CPUalg2.png}}
	\caption{New state computation with different orders}
	\label{fig:cpu_alg}
\end{figure}
We initially made the hypothesis that calculate the new state of a cell entirely based on the old state of the neighbor cells would have worked even better than the current algorithm. This way we would have had a maximum speedup achievable of the core simulation proportional to the number of the cells.
-> AMDAHL'S LAW WITH PERCENTAGE.
Given our lack of competence in this field, we contacted Professor Konrad Walus of the University of British Columbia (Canada), which is the head of the Microsystems and Nanotechnology Research Group (MiNa) which developed this tool. He sustained our hypothesis, adding that their implementation was different just because copying each value would have worsen considerably performances on CPU, and that any numerical problem that may incur is bypassed by randomizing each sample the order in which cells are considered.\newline
As far as coherence engine is concerned, this was true, and the team which is working on it had brilliant results with this algorithm. Unfortunately, the bistable simulation gave us an unexpected result: the state of the system did not converge to a stable state, on the contrary, cells' state starts oscillating and oscillations seems to get larger after some iterations. This result is probably due to the fact that this kind of simulation is just an approximation of the dynamic of the system and what happens is something comparable to the latch SR when is goes from the \textit{forbidden state} $R=S=1$ to the \textit{keep state} $R=S=0$, whose output is unpredictable and the result will depend on the propagation time relations between the NOR gates. In our case, the algorithm is trying to compute the new state not considering any analogue relaxation time difference. Obviously this problem occur only when making logical approximations. The coherence engine is a physical simulation, so this problem is overcome.

\subsection{Parallelization by labels}
In order to find a working parallelizable algorithm, two approaches had been considered then. The former was to modify someway the computation of the new state, in order to overcome any sort of oscillation and maintain the before prospected speedup roof, the second was to consider the dependency that was discussed in section \ref{sec:new_algorithm}, affecting negatively the achievable speedup.
As for the first option, we tried adding some noise or skipping randomly the new polarization update when an oscillation is individuated, but we did not succeed. Again because of our lack of competence, we showed this problem to the MiNa group and we are currently in contact with a PhD which actively contributed to the creation of this tool. Meanwhile we managed to obtain a working bistable simulator exploiting the second option.\newline
In order to maintain the dependency of the original algorithm we decided to divide the entire circuit into labels so that every cell has no neighbors on his label and computing simultaneously not all the cells, but cells belonging to the same label.
The neighborhood relationship is stored as an undirected graph. Applying a coloring algorithm on it, we obtained these labels. Our graph has some peculiarity which had to be considered in order to find a fast and efficient coloring algorithm:
\begin{itemize}
	\item The graph is undirected, but as for \textit{fixed} and \textit{input} cells (whose values do not depend on the other cells state) it is directed, since we decided to mark these kind of cells as if they have no neighbors, while a normal cell is allowed to have them as neighbors;
	\item A \textit{fixed} and \textit{input} cell is allowed to have any color
\end{itemize}
This way a sequentiality is introduced and the maximum speedup achievable is inevitably worsen.

-> COLORING
-> NUOVO SPEEDUP: CELLE/COLORS

\subsection{Data Structures}
In the CPU implementation all the information about circuit structure and cells' details are stored in complex nested and dereferenced structures:
considering CUDA SIMT parallelism, well suited to operate over structures like matrices and arrays, we first detected the useful data
for simulation on GPU and then decided how to organize them for a fast CUDA implementation.\newline
The essential data we identified were: cells' polarizations, neighbors'lists, intra-cells kinetic energy (\textit{eK}) values, clock values,
stability status and other predefined constants.\newline
We decided to store most of these values (the ones that ) in simple array structures, in order to obtain a clearer thread mapping and a faster thread access.\newline
Cells' polarizations array is in charge of containing the polarization values for all the cells through all the iterations, providing
the results for simulation output. At each iteration, polarizations are updated by threads.\newline
Neighbors' and \textit{eK} arrays contain all the informations we need about circuit structure and energy interactions among
the cells. They fully depend on circuit's geometry, and don't change during the simulation. However, they are essential to calculate 
cells polarizations.\newline
Stability array contains simple boolean values which provide the stability status of every cell and permits the evaluation of array global
stability after every iteration.\newline
Other significant structures we designed in our implementations are the input and output cells' indexes arrays, two auxiliary structures
which help us respectively to update input cells at the beginning of every sample and to store output cells' polarization at the end of 
every iteration.

\subsection{Parallel Algorithm}
As reported previously, in our parallel implementation we focused our attention on the simulation core of the algorithm.\newline
Our goal was to exploit the parallel thread execution to compute simultaneously all the circuit's cells. 

BISOGNA SPIEGARE LA ROBA DI KONRAD CHE CI HA FATTO PENARE, MA POI NE SIAMO USCITI ABILMENTE CON IL MIGLIE'S COLORING (SPIEGATO NELLA SEZIONE SOPRA)

After the initial circuit file reading and structures filling phases, we introduced a conversion function to have data in our desired format.
After conversion the coloring algorithm is applied and all the needed data are ready to be allocated on the device.\newline
Parallel bistable simulation, as showed in (ref algoritmo) could be divided in stages below:
\begin{itemize}
 \item allocation and copy of the variables on the device, which is performed only one time during the execution.
 \item update inputs kernel, executed at the beginning of each sample to update inputs.
 \item bistable kernel, invoked in every iteration as many times as the number of colors in the circuit
 \item stability checking, performed after each iteration
 \item copy-back of output cells polarizations from device to host, necessary to save circuit output at the end of each sample
 \item cleaning device memory after the last sample.
\end{itemize}


SI POTREBBE FARE UN BELL'ALGORITMO SCRITTO BENE SULLO PSEUDOCOICE DELL'IMPLEMENTAZIONE.. NEL FILE CHE HO INCLUSO CI SONO DUE ESEMPI
TRATTI DALLA MIA TESI
\input{parallel_algorithm}





\subsection{Memory allocation}
As reported in the section \refname{\label{sec:art_of_cuda}}, when engeneering an algorithm for the GPUs, a critical
design decision is the allocation of data onto the different memories provided by the architecture.
Almost all the structures above have changeable sizes, depending on the circuit structure (number of cells and neighbourhood among them).
For that reason array dimension may ramp up to many thousands of elements, dramatically increasing the space needed on the device.
While Tesla c1060's 4GB global memory is safely enough to store these structures, the same cannot be stated for the shared and constant
memories: these have very limited storage space and need to be exploited very carefully.
We decided to use shared memory to store relatively small structures such as the input and output indexes arrays. The input array is used
in the \textit{update input kernel} to fastly find the input cells to be updated. In the same way, output indexes array is exploited 
in the \textit{bistable kernel} to store output cells' polarizations.\newline
The reasons of such a choice consist both in the limited sizes of these structures and in the high frequency of accesses to them during
execution.\newline
In the \textit{constant memory} cache we allocated all the necessary variables which not vary during the execution and need to be accessed
by all the threads, such as clock constants, number of cells (input, output and total amount), stability tolerance, number of samples 
 maximum number of neighbours. All the remaining variables are stored in thread's local registers.\newline

\subsection{Optimizations}
*shared memory (GIÀ CITATA)
*coalescence (HO FATTO UN PROFILING, SUL CIRCUIT_2_04 E SI VEDE CHE CI SONO UN BEL PO' DI ACCESSI NON COALESCENTI)










