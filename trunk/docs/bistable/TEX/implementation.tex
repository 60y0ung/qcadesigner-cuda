\chapter{Implementation}\label{sec:implementation}
\section{First approach}
The original source code of QCADesigner was downloaded from Mina website (\cite{site:QCADesigner}). We attempted to make it compile as it was but we did not
manage to solve several compilation errors. So we started to focus on the identification of the core algorithm supporting the tool in
order to obtain a working batch simulator executable on CPU. Meanwhile we were able to deeply
analyze the code. We made some hypothesis on the location of possible bottlenecks, we identified the data structures used to represent 
circuits and started to consider possible transformations that had to be done in order to obtain fast accessible and light weight data 
structures allocable on the GPU global memory.

\section{The CPU algorithm and code analysis}\label{sec:cpu_algorithm}
Bistable engine is thought as a fast and approximated simulation, sufficient to verify the logic functionality of a design. 
Every cell is represented as a simple two-state system. The algorithm \ref{algo_bist_serial} shows a pseudo code of the simulation core. 
\input{serial_algorithm}
The entire evolution of the system is divided into samples, that are units of time (not yet experimentally defined). At the beginning of each sample, input values are updated and cells order is randomized. The state of each cell is then calculated with respect to the other cells within 
a preset effective radius. This operation is iterated until all cells have converged within a predetermined $tolerance$ (\ref{algo_bist_serial}, line \ref{alg:tolerance}). 
Once the entire system converges the output is recorded and the computation goes on with the next sample.
The number of samples required to have a good approximation is known (\cite{site:MinaBistable}) to be about $1000*d+1000*k$, where $d$ is the circuit delay (number of clocks) and $k$ is the number of inputs combinations. So far, the tool only allows exhaustive simulations, that is, all possible combinations of inputs are evaluated, and the procedure is performed twice, since the first $d$ cycles output values are meaningless. Thus, $2000*2^N$ samples are needed, where $N$ is the number of input cells. Obviously, this parameter introduces an exponential complexity which is just related to the fact that the number of all possible combinations of inputs is $2^N$.
 The maximum number of iterations allowed for the convergence within a sample is set to $100$. Thus, during a simulation each cell's value 
is computed sequentially $It*2000*2^N$ times, where $It$ is the mean number of iterations needed to reach convergence. 
The more complex is the circuit, the longer will take each sample to reach convergence. Each cell new state calculation implies several accesses to the memory, since data structures are heavily dereferenced, and each neighbor's state as well as their reciprocal kink energy has to be read, and a series of floating point operations on values with double precision. Furthermore, at the beginning of each sample, new inputs values have to be set. As far as exhaustive simulation is concerned, every combination of input values has to be processed. Therefore, their values are calculated throughout a periodic function, implying several transcendental functional unit usages.\newline
The analysis of the code makes it quite clear that the core of the simulation is the main bottleneck of this application. Once a circuit of millions of cells have to be simulated, the number of FP operations and memory accesses reach huge orders of magnitude. This first hypothesis was subsequently proved by the profiling of execution times, dealt with in section \ref{sec:cpu_profiling}.


\section{Profiling of CPU simulation}\label{sec:cpu_profiling}
We made the batch simulator work and had the CPU simulation profiled. We simulated some circuits of different sizes and different number of inputs and took the execution times. The table \ref{tab:cpu_profiling} shows the results we obtained. We calculated the mean number of iterations per sample needed to reach convergence, the core simulation time, the time of the entire simulation (including data loading from file and creation of data structures) and the proportion of the computation that we aim to speedup.
\newline
\begin{table}[h!tb]
   \centering \caption{CPU simulation Profiling}
   \label{tab:cpu_profiling}
   \vskip 0.2cm
   %%
   \scalebox{0.90}{
	    %% The {|c|c|c|c|c|} define the number of columns.
	    %% c means centered
	    %% | defines a vertical line between two columns 
	    \begin{tabular}{|c|c|c|c|c|c|}
	      \hline
	      Cells & Samples & Iter. per sample & Core Time & Total Time & P \\
	      \hline
	      35576 & 8000 & 10.16 & 2590 s & 3106 s & 83.3 \% \\
	      \hline
	      35486 & 16000 & 12 & 5642 s & 6165 s & 91.5 \% \\
	      \hline
	      31875 & 32000 & 11 & 8668 s & 9078 s & 95,4 \% \\
	      \hline
	      35893 & 64000 & 11 & 20799 s & 21335 s & 97.5 \% \\
	      \hline
	      24498 & 128000 & 11,5 & 25324 s & 25544 s & 99.1 \% \\
	      \hline
	    \end{tabular}
	 }
 \end{table}
As we can see, the hypothesis we made about the bottleneck is verified. Furthermore, the proportion of the computation for large circuits increases with the number of samples used, so with the number of inputs. The maximum expected improvement is, according to Amdahl's Law: $ \frac{1}{(1-P)+\frac{P}{S}} $, where S is the maximum speedup achievable parallelizing the portion P. For large circuits we can consider $P=1$ and thus the speedup of the entire simulation is equal to the speedup we manage to obtain from the parallelization of the core simulation. Whatever time occur to perform the part of the code outside the core cannot be eliminated or improved since it consists of data loading from file and creation of data structures.

\section{Parallelization Strategy}
In this section, we consider the original algorithm in order to find a parallel algorithm which best exploit CUDA technology, making some consideration on achievable speedup and correctness of results.
\subsection{The parallel algorithm and achievable speedup}\label{sec:new_algorithm}
As we have seen in section \ref{sec:cpu_algorithm}, the calculation of each cell new state is immediately stored before proceeding with the next cell. Therefore, not all of the new polarizations computation is based on the old value of neighbors. This dependence challenges our seek of the maximum speedup, since our initial proposal was to compute all the new states of cells simultaneously. Figure \ref{fig:cpu_alg} shows how this dependency affects a simulation step varying the order in which the cells are considered.
\begin{figure}[h!tb]
	\centering
	\subfigure[First order]{\includegraphics[scale=0.4]{img/CPUalg1.png}}
	\subfigure[Second order]{\includegraphics[scale=0.4]{img/CPUalg2.png}}
	\caption{New state computation with different orders}
	\label{fig:cpu_alg}
\end{figure}
We initially made the hypothesis that calculate the new state of a cell entirely based on the old state of the neighbor cells would have worked even better than the current algorithm. This way we would have had a maximum speedup achievable of the core simulation proportional to the number of the cells.
Professor Konrad Walus of the University of British Columbia (Canada), which is the head of the Microsystems and Nanotechnology Research Group (MiNa) which developed this tool, sustained our hypothesis, adding that their implementation was different just because copying each value would have worsen considerably performances on CPU, and that any numerical problem that may incur is bypassed by the randomization performed each sample (\ref{algo_bist_serial} line \ref{alg:rand}) to the order in which cells are considered. Algorithm \ref{algo_bist_parallel_failure} shows the pseudo code of our first proposal.
\input{parallel_algorithm_failure}
BistableKernel (\ref{alg:algo_bist_parallel_failure} line \ref{alg:bistkernel}) is the part of the code that has to be computed by each GPU thread. Each thread is in charge of computing the new state of each cell based on the values of his neighbors. The number of the threads that will simultaneously will be executed is dependent on the architecture of the device.
Unfortunately, the bistable simulation gave us an unexpected result: the state of the system did not converge to a stable state, on the contrary, cells' state starts oscillating and oscillations seems to get larger after some iterations. This result is probably due to the fact that this kind of simulation is just an approximation of the dynamic of the system and what happens is something comparable to the latch SR when is goes from the \textit{forbidden state} $R=S=1$ to the \textit{keep state} $R=S=0$, whose output is unpredictable and the result will depend on the propagation time relations between the NOR gates. In our case, the algorithm is trying to compute the new state not considering any analogue relaxation time difference. Obviously this problem occur only when making logical approximations. The coherence engine is a physical simulation, so this problem is overcome.

\subsection{Parallelization by labels}
In order to find a working parallelizable algorithm, two approaches had been considered then. The former was to modify someway the computation of the new state, in order to overcome any sort of oscillation and maintain the before prospected speedup roof, the second was to consider the dependency that was discussed in section \ref{sec:new_algorithm}, affecting negatively the achievable speedup.
As for the first option, we tried adding some noise or skipping randomly the new polarization update when an oscillation is individuated, but we did not succeed. Again because of our lack of competence, we showed this problem to the MiNa group and we are currently in contact with a PhD which actively contributed to the creation of this tool. Meanwhile we managed to obtain a working bistable simulator exploiting the second option.\newline
\input{parallel_algorithm}
In order to maintain the dependency of the original algorithm we decided to divide the entire circuit into labels so that every cell has no neighbors on his label and computing simultaneously not all the cells, but cells belonging to the same label.
The neighborhood relationship is stored as an undirected graph. Applying a coloring algorithm on it, we obtained these labels. Our graph has some peculiarity which had to be considered in order to find a fast and efficient coloring algorithm:
\begin{itemize}
	\item The graph is undirected, but as for \textit{fixed} and \textit{input} cells (whose values do not depend on the other cells state) it is directed, since we decided to mark these kind of cells as if they have no neighbors, while a normal cell is allowed to have them as neighbors;
	\item A \textit{fixed} and \textit{input} cell is allowed to have any color;
	\item A normal cell is allowed to have the same color of his neighbor if this neighbor is a \textit{fixed} and \textit{input} cell;
	\item The maximum number of arcs does not scale with the number of cells since there is a predefined effective radius within which the number of cells that can be located is physically bounded.
\end{itemize}
Therefore, we preferred to implement a home made coloring algorithm, which has a linear complexity and gives a small number of colors in a really short time. The graph \ref{fig:color_table} shows the trend of the number of colors found in some circuits in relation to the number of cells.
\begin{figure}[h!tb]
				\centering
				\includegraphics[scale=0.5]{img/color_table.png}
				\caption{Number of colors found with our coloring algorithm.}
				\label{fig:color_table}
\end{figure}
Obviously it also depends on the structure of the entire circuit, but here is shown how the last property we pointed out makes the 
maximum number of colors well bounded by a limit of about 20-25 colors.\newline In order to avoid numerical errors, the randomization 
of the cells' order would be too time expensive because of the many memory transfers that would be required between host and device. 
With this new algorithm, things become easier since it is sufficient to randomize the color order every sample so that the there is no
 predefined sequence of labels being computed.\newline
This way a sequentiality is introduced and the maximum speedup achievable is inevitably worsen, but still our maximum speedup will be 
proportional to the number of cells as the circuit get larger, since we can consider as a constant the number of colors.

\subsection{Data Structures}
\subsubsection{Original Structures}
In the CPU implementation all the information about circuit structure and details of cells are stored in complex nested and dereferenced
 structures. \lstlistingname~\ref{bistable_struct} shows how in the bistable model every cell is represented by a C struct containing 
the the most useful features for the simulation.
The first four fields of the structures provide all the necessary informations about relationships with other cells. The integer variable 
\texttt{number\_of\_neighbours} allow to iterate correctly on other arrays to execute the needed operations.
The combined use of \texttt{neighbours} and \texttt{neighbour\_layer} pointers permits to reach neighbors cells while \texttt{Ek} points
to the array array containing all the kink energy values with respect to the neighbors cells.\newline
The new polarization of the cell is then computed performing a set of operations over the polarizations of the neighbors, the respective 
kink energy and the current cell polarization.\newline
\begin{lstlisting}[caption=Bistable model structure for the cell, label=bistable_struct]
	    typedef struct
	      {
	      int number_of_neighbours;
	      QCADCell **neighbours;
	      int *neighbour_layer;
	      double *Ek;
	      double polarization;
	      } bistable_model;
\end{lstlisting}
These cell structs are actually only a part of a more complex nested structure: the \textit{QCADCell}, which, for the sake of simplicty 
we are not going to describe in this paper. These \textit{QCADCell} structures are respectively grouped by layers in a matrix of sorted 
cells. However 

\subsubsection{New Data Structures}
Original implementation structures are very complicated and sovrabundant of attributes: most of the data stored in them are useless with 
respect to our scope. Moreover the nested structure is clearly unsuitable to the CUDA implementation: in GPGPU programming is 
recommended to employ simple data structures such as arrays and matrices to exploit the SIMT parallelism. 
 As first step in the creation of more we extracted all the useful features to execute the core simulation.
The essential data we identified were: polarizations of cells, lists of neighbors, intra-cells kinetic energy (\textit{EK}) values, clock 
values, stability status and other predefined constants. Most of them are organized in the structure described in 
\lstlistingname~\ref{bistable_struct}.\newline
We decided to store most of these values in simple array structures, in order to obtain a clearer thread mapping and a faster thread 
access. By means of a conversion function we itarate over the cell matrix to extract, separate and allocate the desired data.\newline
Polarization array is in charge of containing the polarization values for all the cells through all the iterations, providing
the results for simulation output. At each iteration, polarizations are updated by threads.\newline
Neighbors and \textit{eK} arrays contain all the informations we need about circuit structure and energy interactions among
the cells. They fully depend on the geometry of circuit, and do not change during the simulation. However, they are essential to calculate 
new polarizations of cells.\newline
Stability array contains simple boolean values which provide the stability status of every cell and permits the evaluation of array global
stability after every iteration.\newline
Other significant structures we designed in our implementations are the arrays of indexes of input and output cells, two auxiliary structures
which help us respectively to update input cells at the beginning of every sample and to store output cells' polarization at the end of 
every iteration.\newline




\subsection{Memory allocation}
As reported in the section \ref{sec:art_of_cuda} , when engeneering an algorithm for the GPUs, a critical
design decision is the allocation of data onto the different memories provided by the architecture.
Almost all the structures above have changeable sizes, depending on the circuit structure (number of cells and neighbourhood among them).
For that reason array dimension may ramp up to many thousands of elements, dramatically increasing the space needed on the device.
While Tesla c1060's 4GB global memory is safely enough to store these structures, the same cannot be stated for the shared and constant
memories: these have very limited storage space and need to be exploited very carefully.
We decided to use shared memory to store relatively small structures such as the input and output indexes arrays. The input array is used
in the \textit{update input kernel} to find the input cells and fastly update them. In the same way, output indexes array is exploited 
in the \textit{bistable kernel} to store polarizations ov output cells.\newline
The reasons of such a choice consist both in the limited sizes of these structures and in the high frequency of accesses to them during
execution. In the extreme case of a number of input or output cells exceeding the maximum size of the available shared memory per thread, 
the respective array is normally allocated in global memory.\newline
In the \textit{constant memory} cache we allocated all the necessary variables which not vary during the execution and need to be accessed
by all the threads, such as clock constants, number of cells (input, output and total amount), stability tolerance, number of samples 
 maximum number of neighbors. Although the indexes of neighbors  and the \textit{Ek} arrays ramain fixed during simulation execution, they 
cannot be allocated safely on the limited \textit{constant} memory cache because they exceed very easily the maximum available space of
65 \textit{KBytes}. All the remaining variables are stored in the local thread registers.\newline

\subsection{Optimizations}
\label{sec:optimizations}
In Order to best exploit Tesla parallel execution we had to look to some typical GPU programming's expedients.
As we described above, we first focused our attention on the better memory allocation for all the needed variables.
\begin{itemize}
 \item Trying to exploit \textbf{fast access memories} is one of the most valuable efforts to improve performances.
In fact the global memory can be potentially 150x slower than registers, local and shared memory.\newline
Unfortunately in our case data size problems led us to limit the use of \textit{shared} memory to store the input/output indexes arrays and  the
\textit{constant} memory to store predefined variables. However, that turned out in sensible improvements in both the cases of the first kernel and
the second kernel.\newline 
Register,which are employed to store local thread variables, represent a structural limit to the number of thread blocks 
instantiated in every SMP. Tesla provides 16384 32 bits registers per SMP and our implementation requires 10 registers per-thread: by means of the 
apposite occupancy calculator we verified that every thread blocks in execution need 2560 registers per SMP.
Considering that the maximum number of active 256 thread blocks in execution on a SMP is limited to 4, we verified that the maximum 
number of registers used (10240) does not exceed the number of registers provided by the architecture. Moreover, we assessed that the limit
of thread blocks for each Streaming Multiprocessor is given by the number of warps expected by our implementation.

\begin{figure}[h!tp]
    \centering
     \includegraphics[width=0.8\textwidth]{./img/Divergence}
\caption{Architectural detail of the Tesla Architecture}\label{fig:divergence}
    \end{figure}


\item A key issue for all the CUDA applications is the bottleneck related to the \textbf{memory transfers} between host and device.
We reduced these low bandwidth transfers to the minimum: we decided to allocate all the needed structures once at the beginning of
the host function and then to recompute, when possible, data directly on the GPU without the need of passing through CPU. On chip memory transfers are
obviously faster, but we could not avoid to perform some fundamental device-to-host data transfers, such as the stability array and the output
array, respectively done after each iteration and after each sample.


\item \textbf{Coalescence} is another best practice improvement for CUDA code: \textit{global memory} delivers the highest memory 
bandwidth only when the global memory accesses can be coalesced within a half-warp so the hardware can then fetch (or store) the data
 in the fewest number of transactions. If the memory transaction cannot be coalesced, then a separate memory transaction will be issued 
for each thread in the half-warp, which is undesirable. We tried to design thread accesses to be as much coalescing as possible, but it 
was not possible to avoid uncoalescent accesses in instructions such as the polarization calculation (where the access to polarization 
array has no regular pattern, as shown in \figurename~\ref{fig:divergence}). Fortunately newer architectures such as Tesla c1060 have more relaxed coalescing requirements, and 
multiprocessors are more able to deal with conflicts in memory accesses reducing multiple transaction penalty.\newline
\figurename~\ref{fig:Coalescence} shows how we modified the disposition of data in the array of indexes of neighbors (\figurename~\ref{fig:CoalescA}) 
with respect to the possible coalescing access patterns allowed by the Tesla Architecture(\figurename~\ref{fig:CoalescB}).\newline
However, considering the forthcoming distribution of the compute capability 2.0 devices (with a completely cached global memory)
coalescence will be no longer a GPU programmers' issue.

\begin{figure}[h!tb]
	\centering
	\subfigure[Tesla allowed coalescing accesses]{\includegraphics[scale=0.4]{img/coa4.png}}\label{fig:coalescA}
	\subfigure[Array of neighbours indexes access pattern]{\includegraphics[scale=0.4]{img/coalescence.png}}\label{fig:coalescB}
	\caption{Comparation between the Tesla architecture coalescence requirements and our rearrangement of the array of neighbours}
	\label{fig:Coalescence}
\end{figure}




\end{itemize}










