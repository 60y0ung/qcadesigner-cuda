**ABSTRACT**
2 righe sul lavoro, ->VENDITI BENE

**THE TEAM**
Miglie: analisi del codice di partenza, correzione e adattamento del codice per ottenere il batch simulator, controllo e creazione degli algoritmi per la correttezza dei risultati, debugging.
Gibbo: analisi del codice, delle strutture dati necessarie e loro trasformazione in strutture ottimizzate per cuda, profiling iniziale del codice e individuazione del bottleneck.
Marconi: analisi del codice e dell'algoritmo per l'individuazione degli elementi parallelizzabili, creazione del codice cuda e sua compilazione, ottimizzazione e profiling su cuda, debugging.

**STATE OF THE ART**
- QCA 
	* QCADesigner
	* Two Engines: BISTABLE and COHERENCE, describe them shortly (see MINA site)
	-> BISTABLE IS JUST A FAST APPROXIMATION to test circuits
- CUDA

**RATIONALE**
- Why QCA?
	* novel emerging paradigm
	* 2 Thz, low energy consumption and miniaturization
	* quantum computing
- QCADesigner simulator slow on big circuits:
	* Every sample, each cell's polarization is computed based on the values of his neighbors, sequentially.
	* Bottleneck from profiling (table with times)
	* Simulation core: pseudo code
	* Identical operations repeated for each cell, big circuits -> thousands of cells
	* We chose to speedup this part of the code with CUDA because:
		^ SIMD architecture (SIMT): single instruction repeated on different data
		^ hundreds of core -> many threads running simultaneously, each thread responsible of computing a cell's polarization
		^ scalable, adding new cores implies a greater number of cells computed simultaenously, higher speedup
- Objective
	* Speed up simulation for big circuits
	* batch simulator
	* Given a file .qca -> produce output: binary, continous values, plot on png, log with info of simulation
	* if same .qca -> same results CPU and CUDA

**IMPLEMENTATION**
- First approach
	* Downloaded from MINA the latest version of QCADesigner, NOT COMPILES!
	* At first, lot of work done to obtain a working batch simulator on CPU
	* Meanwhile analisys of the code, location of possible bottlenecks, analisys of data structures and their possible transformation in order to obtain best explotation of CUDA
	* Batch simulator on CPU ready -> start profiling (table) and location of bottleneck.
- CUDA implementation
	* CPU algorithm -> CUDA algorithm proposed (working on old values each iteration, with Konrad's blessing): pseudo code.
	* After first implementation: wrong results, cells' polarizations don't converge, oscillations.
	-> Bistable approximations doesn't work with this algorithm.
	-> Solution: Don't change CPU algorithm -> don't compute neighbour cells values simultaneously.
	-> parallelization? Coloring algorithm
	-> randomization? randomize color order each sample
	* Cuda main data structures: arrays of polarizations, neighborhood, clocks, kink energies, stability.
	* Memory occupation on global, coalescent accesses, only one vector for old and new polarization
	* Constant memory, variables
	* shared memory for arrays of indexes of inputs and outputs (many accesses during kernel execution)
	* Moved clock values & input values calculation (only when they change) inside the kernel
	* Memory transfers
	* Fast math and float for faster approximation (only one double FU per SM)
	* Compulsory divergence reading neighbours
- Discussion pre-results on core simulation:
	* Iterations per sample: ~5-10
	* Colors: ~ 15-20
	* Number of samples: 2000*2^number_of_inputs
	* On CPU complexity: O(samples x mean iterations per sample x cells) ~ O(2000*2^Ninput x 10 x Ncells) = O(C*2^N)
	* Max (theoretical) speedup achievable = cells/colors -> O(sample x mean iterations per sample x colors) ~ O(2000*2^Ninputs x 10 x 15) = O(2^N)
	-> Technology constraints:
	* Memory transfers:
		^ each iteration: device->host, stability: cells x 1 byte
		^ each sample: device->host, outputs: number_of_outputs x 8 bytes
		^ main_kernel calls: samples x mean iterations per sample x colors
		^ update inputs kernel calls: 2 x 2^number_of_inputs (once every 1000 samples)
		^ main kernel:
			- 1 global coalescent -> shared : number_of_outputs x 8 bytes (output_indexes)
			- 3 x global read coalescent : 3 x number_of_cells x 4 bytes (neighbours, cells_colors, cells_clock)
			--- for cycle: ( 2 coalescent + 1 random ) x max number of neighbours -> big divergency
			- 1 gloabal read + 2 global write coalescent: number_of_cells x (8 + 1 + 8) bytes (polarization, stability, polarization)
			- number_of_outputs reads in shared
			- write on global non coalescent
		^ transfer rate?

**RESULTS**

**CONCLUSIONS**