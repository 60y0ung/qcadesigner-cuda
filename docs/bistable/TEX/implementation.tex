\chapter{implementation}\label{sec:implementation}
\section{First approach}
The original source code of QCADesigner was downloaded from Mina website (ref). We attempted to make it compile as it was but we did not
manage to solve several compilation errors. So we started to focus on the identification of the core algorithm supporting the tool in
order to obtain a working batch simulator executable on CPU. This operation took us some weeks of work. Meanwhile we were able to deeply
analyze the code. We made some hypothesis on the location of possible bottlenecks, we identified the data structures used to represent 
circuits and started to consider possible transformations that had to be done in order to obtain fast accessible and light weight data 
structures allocable on the GPU global memory.

\section{CPU algorithm and profiling}
Bistable engine is thought as a fast and approximated simulation, sufficient to verify the logic functionality of a design. 
Every cell is represented as a simple two-state system. The entire simulation is divided into samples, that are units of time 
(not yet experimentally defined), and for each sample the state of each cell is calculated with respect to the other cells within 
a preset effective radius. This operation is iterated until all cells have converged within a predetermined tolerance. 
Once the entire system converges the output is recorded and the computation goes on with the next sample, after having updated
 the input cells with new input values.
The number of samples required to have a good approximation is known (ref) to be $2000*2^N$, where $N$ is the number of inputs.
 The maximum number of iterations allowed for the convergence within a sample is $100$. Thus, during a simulation each cell's value 
is computed sequentially $It*2000*2^N$ times, where $It$ is the mean number of iterations needed to reach convergence. 
The more are the cells, the longer will take each sample to reach convergence.

Once we finished the batch application we started to profile the execution times simulating some circuits. The table (ref) shows 


\section{Parallelization Strategy}

\subsection{Data Structures}
In the CPU implementation all the informations about circuit structure and cells' details are stored in complex nested structures:
considering CUDA SIMT parallelism, well suited to operate over structures like matrices anda arrays, we first detected the useful data
for simulation on GPU and then decided how to organize them for a fast CUDA implementation.\newline
The essential data we identified were: cells' polarizations, neighbours'lists, intra-cells kinetic energy (\textit{eK}) values, clock values,
stability status and other predefined constants.\newline
We decided to store most of these values (the ones that ) in simple array structures, in order to obtain a clearer thread mapping and a faster thread access.\newline
Cells' polarizations array is in charge to contain the polarization values for all the cells through all the iterations, providing
the results for simulation output. At each iteration, polarizations are updated by threads.\newline
Neighbours' and \textit{eK} arrays contain all the informations we need about circuit structure and energy interactions among
the cells. They fully depend on circuit's geometry, and don't change during the simulation. However, they are essential to calculate 
cells polarizations.\newline
Stability array contains simple boolean values wich provide the stability status of every cell and permits the evaluation of array global
stability after every iteration.\newline
Other significant structures we designed in our implementations are the input and output cells' indexes arrays, two auxiliary structures
which help us respectively to update input cells at the beginning of every sample and to store output cells' polarization at the end of 
every iteration.

\subsection{Parallel Algorithm}
As reported previously, in our parallal implementation we focused our attention on the simulation core of the algorithm.\newline
Our goal was to exploit the parallel thread execution to compute simultaneously all the circuit's cells. 

BISOGNA SPIEGARE LA ROBA DI KONRAD CHE CI HA FATTO PENARE, MA POI NE SIAMO USCITI ABILMENTE CON IL MIGLIE'S COLORING

After the initial circuit file reading and structures filling phases, we introduced a conversion function to have data in our desired format.
After conversion the coloring algorithm is applied and all the needed data are ready to be allocated on the device.\newline
Parallel bistable simulation, as showed in (ref algoritmo) could be divided in stages below:
\begin{itemize}
 \item allocation and copy of the variables on the device, which is performed only one time during the execution.
 \item update inputs kernel, executed at the beginning of each sample to update inputs.
 \item bistable kernel, invoked in every iteration as many times as the number of colors in the circuit
 \item stability checking, performed after each iteration
 \item copy-back of output cells polarizations from device to host, necessary to save circuit output at the end of each sample
 \item cleaning device memory after the last sample.
\end{itemize}


SI POTREBBE FARE UN BELL'ALGORITMO SCRITTO BENE SULLO PSEUDOCOICE DELL'IMPLEMENTAZIONE.. NEL FILE CHE HO INCLUSO CI SONO DUE ESEMPI
TRATTI DALLA MIA TESI
\input{parallel_algorithm}





\subsection{Memory allocation}
As reported in the section \refname{\label{sec:art_of_cuda}}, when engeneering an algorithm for the GPUs, a critical
design decision is the allocation of data onto the different memories provided by the architecture.
Almost all the structures above have changeable sizes, depending on the circuit structure (number of cells and neighbourhood among them).
For that reason array dimension may ramp up to many thousands of elements, dramatically increasing the space needed on the device.
While Tesla c1060's 4GB global memory is safely enough to store these structures, the same cannot be stated for the shared and constant
memories: these have very limited storage space and need to be exploited very carefully.
We decided to use shared memory to store relatively small structures such as the input and output indexes arrays. The input array is used
in the \textit{update input kernel} to fastly find the input cells to be updated. In the same way, output indexes array is exploited 
in the \textit{bistable kernel} to store output cells' polarizations.\newline
The reasons of such a choice consist both in the limited sizes of these structures and in the high frequency of accesses to them during
execution.\newline
In the \textit{constant memory} cache we allocated all the necessary variables which not vary during the execution and need to be accessed
by all the threads, such as clock constants, number of cells (input, output and total amount), stability tolerance, number of samples 
 maximum number of neighbours. All the remaining variables are stored in thread's local registers.\newline

\subsection{Optimizations}
*shared memory (GIÃ€ CITATA)
*coalescence (HO FATTO UN PROFILING, SUL CIRCUIT_2_04 E SI VEDE CHE CI SONO UN BEL PO' DI ACCESSI NON COALESCENTI)










